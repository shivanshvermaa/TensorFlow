{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\n",
    "   'The pigs were insulted that they were named hamburgers.',\n",
    "'He didn’t want to go to the dentist, yet he went anyway.',\n",
    "'It’s a skateboarding penguin with a sunhat!',\n",
    "'I often see the time 11:11 or 12:34 on clocks.',\n",
    "'She cried diamonds.',\n",
    "'She had some amazing news to share but nobody to share it with.',\n",
    "'Seek success, but always be prepared for random cats.',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We will try to assign each word a unique token so that it can be uniquely be identified \n",
    "#It will automatically remove all the exclamation and other punctuations\n",
    "tokenizer = Tokenizer( num_words = 100  )\n",
    "tokenizer.fit_on_texts(sentences)\n",
    "word_index = tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'to': 1, 'the': 2, 'were': 3, 'he': 4, 'it': 5, 'a': 6, 'with': 7, '11': 8, 'she': 9, 'share': 10, 'but': 11, 'pigs': 12, 'insulted': 13, 'that': 14, 'they': 15, 'named': 16, 'hamburgers': 17, 'didn’t': 18, 'want': 19, 'go': 20, 'dentist': 21, 'yet': 22, 'went': 23, 'anyway': 24, 's': 25, 'skateboarding': 26, 'penguin': 27, 'sunhat': 28, 'i': 29, 'often': 30, 'see': 31, 'time': 32, 'or': 33, '12': 34, '34': 35, 'on': 36, 'clocks': 37, 'cried': 38, 'diamonds': 39, 'had': 40, 'some': 41, 'amazing': 42, 'news': 43, 'nobody': 44, 'seek': 45, 'success': 46, 'always': 47, 'be': 48, 'prepared': 49, 'for': 50, 'random': 51, 'cats': 52}\n"
     ]
    }
   ],
   "source": [
    "print(word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[26, 32, 38, 39], [42, 27, 18, 19, 1, 1, 2, 21]]\n"
     ]
    }
   ],
   "source": [
    "text_to_test = [\n",
    "    'skateboarding time cried diamonds',\n",
    "    'amazing penguin didn’t want to got to the dentist'\n",
    "]\n",
    "\n",
    "text_rep = tokenizer.texts_to_sequences(text_to_test)\n",
    "print(text_rep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'OOV': 1, 'to': 2, 'the': 3, 'were': 4, 'he': 5, 'it': 6, 'a': 7, 'with': 8, '11': 9, 'she': 10, 'share': 11, 'but': 12, 'pigs': 13, 'insulted': 14, 'that': 15, 'they': 16, 'named': 17, 'hamburgers': 18, 'didn’t': 19, 'want': 20, 'go': 21, 'dentist': 22, 'yet': 23, 'went': 24, 'anyway': 25, 's': 26, 'skateboarding': 27, 'penguin': 28, 'sunhat': 29, 'i': 30, 'often': 31, 'see': 32, 'time': 33, 'or': 34, '12': 35, '34': 36, 'on': 37, 'clocks': 38, 'cried': 39, 'diamonds': 40, 'had': 41, 'some': 42, 'amazing': 43, 'news': 44, 'nobody': 45, 'seek': 46, 'success': 47, 'always': 48, 'be': 49, 'prepared': 50, 'for': 51, 'random': 52, 'cats': 53}\n"
     ]
    }
   ],
   "source": [
    "#This model will however not work well with out of dictionary words so we try to incorporate them too in the Tokenizer model\n",
    "tokenizer_new = Tokenizer( num_words = 100 , oov_token = 'OOV') \n",
    "tokenizer_new.fit_on_texts(sentences)\n",
    "word_index_new = tokenizer_new.word_index\n",
    "print(word_index_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 1, 1, 27]]\n"
     ]
    }
   ],
   "source": [
    "new_test_text = [\n",
    "    'Walter white is skateboarding'\n",
    "]\n",
    "text_repnew = tokenizer_new.texts_to_sequences( new_test_text )\n",
    "print(text_repnew)\n",
    "#OOV will be marked with 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0  0  0  0  2 12  3 13 14 15  3 16 17]\n",
      " [ 0  4 18 19  1 20  1  2 21 22  4 23 24]\n",
      " [ 0  0  0  0  0  5 25  6 26 27  7  6 28]\n",
      " [ 0 29 30 31  2 32  8  8 33 34 35 36 37]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  9 38 39]\n",
      " [ 9 40 41 42 43  1 10 11 44  1 10  5  7]\n",
      " [ 0  0  0  0 45 46 11 47 48 49 50 51 52]]\n"
     ]
    }
   ],
   "source": [
    "#Padding \n",
    "#often we will get sentences that are not of the same length so we will create padding for them \n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "sequences = tokenizer.texts_to_sequences(sentences)\n",
    "padding = pad_sequences(sequences)\n",
    "print(padding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
